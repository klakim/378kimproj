---
title: "eda.qmd"
format: html
---

## Quarto

## Penguin Size and Mass by Sex and Species

```{r}
#Needed Packages
install.packages("palmerpenguins")
install.packages("dplyr")
install.packages("dbplyr")
install.packages("ggplot2")
install.packages("DBI")
install.packages("duckdb")
install.packages("vetiver")
install.packages("tidyverse")
install.packages("ISLR2")
install.packages("rpart.plot")
install.packages("vip")
install.packages("randomForest")
install.packages("tidymodels")
install.packages("xgboost")


con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "my-db.duckdb")
DBI::dbWriteTable(con, "penguins", palmerpenguins::penguins, overwrite = TRUE)
```

```{r}
#Load packages 
library(palmerpenguins)
library(dplyr)
library(dbplyr)
library(ggplot2)
library(duckdb)
library(DBI)
library(vetiver)
library(pins)
library(tidyverse)
library(tidymodels)
library(ISLR2)
library(rpart.plot)
library(vip)
library(xgboost)
library(randomForest)



#load data 
con <- DBI::dbConnect(
  duckdb::duckdb(), 
  dbdir = "my-db.duckdb"
  )
df <- dplyr::tbl(con, "penguins")
```

```{r}
#From Chapter 1
df %>%
  group_by(species, sex) %>%
  summarise(
    across(
        ends_with("mm") | ends_with("g"),
      \(x) mean(x, na.rm = TRUE)
      )
    ) %>%
  dplyr::collect() %>%
  knitr::kable()
```

```{r}
#Given EDA
df %>%
  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

## Penguin Bill Length by Sex and Species

```{r}
# Boxplot for comparison 
df%>%
  #formula 
  ggplot(aes(x=species, y = bill_length_mm, fill = species))+ 
  #creation
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Comparison of Bill Length by Species ", x = "Species", y = "Bill Length (mm)")

```

```{r}
# Boxplot for comparison 
df%>%
  #formula 
  ggplot(aes(x=sex, y = bill_length_mm, fill = species))+ 
  #creation
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Comparison of Bill Length by Sex ", x = "Sex", y = "Bill Length (mm)")
```

## Penguin Bill Length by Interaction of Sex and Species

```{r}
df %>%
  ggplot(aes(x = interaction(species, sex), y = bill_length_mm, fill = species)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(
    title = "Comparison of Bill Length by Sex and Species",
    x = "Species and Sex",
    y = "Bill Length (mm)",
    fill = "Species"
  )

```

```         
Random Forest for Classification by Species
```

```{r}
head(penguins)
```

```{r}
#Random forest for classification for species
tree_spec <- decision_tree() %>%   
  set_engine("rpart")
class_tree_spec <- decision_tree() %>%      
  set_engine("rpart") %>%      
  set_mode("classification")

class_tree_fit <- class_tree_spec %>%    
  fit(species ~ ., data = penguins)


class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()

augment(class_tree_fit, new_data = penguins) %>%
  accuracy(truth = species, estimate =.pred_class)
```

---
#From this, the overall accuracy is 96.5% which is pretty good. Let's try to see if there is any change when using a valdiation split. 
---

```{r}
set.seed(1234)
Penguin_split <- initial_split(penguins)

```

```{r}
#Train and test
Penguin_train <- training(Penguin_split)
Penguin_test <- testing(Penguin_split)
```

```{r}
#Fit on just the training data set
class_tree_fit <- class_tree_spec %>%    
  fit(species ~ ., data = Penguin_train)   

class_tree_fit
```

```{r}
augment(class_tree_fit, new_data = Penguin_train) %>%
  conf_mat(truth = species, estimate = .pred_class)
```

```{r}
augment(class_tree_fit, new_data = Penguin_train) %>%
  accuracy(truth = species, estimate = .pred_class)
```

```{r}
augment(class_tree_fit, new_data = Penguin_test) %>%
  conf_mat(truth = species, estimate = .pred_class)

```

```{r}
augment(class_tree_fit, new_data = Penguin_test) %>%
  accuracy(truth = species, estimate = .pred_class)
```

```{r}
#The testing data doesn't perform as well and we get a lower accuracy by around 4%. Let's try to use the cost_complexity option to see if there is a better tree size that will reduce the overfitting that is happening. 
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_formula(species ~ .)
```

```{r}
#With this, we need to use k-fold cross validation and a grid of values 
set.seed(1234)
Penguin_vfold <- vfold_cv(Penguin_train, v = 10, strata = species)
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```

```{r}
#Fitting the model
#| cache: true 
tune_res <- tune_grid(
  class_tree_wf, 
  resamples = Penguin_vfold, 
  grid = param_grid, 
  metrics = metric_set(accuracy)
)
```

```{r}
#Which cost-complexity results in the highest accuracy 
#| warning: false 
autoplot(tune_res)
best_complexity <- select_best(tune_res)
```

```{r}
#Update the cost_complexity and on the full training set 
#| warning: false 
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = Penguin_train)
class_tree_final_fit
```

```{r}
#Final pruned tree 
class_tree_final_fit %>%      
  extract_fit_engine() %>%      
  rpart.plot()
```

```{r}
#Best Accuracy 
augment(class_tree_fit, new_data = Penguin_train) %>%
  accuracy(truth = species, estimate = .pred_class)
```

```{r}
#The output of this is the pruned tree
class_tree_final_fit %>%      
  extract_fit_engine() %>%      
  rpart.plot()
```

```{r}
DBI::dbDisconnect(con, shutdown = TRUE)
```
